---
title:  '\begin{center} Predictive Analytics: Credit Risk Scorecard Application \\ Case Study: Group 15 \\  \end{center}'
author:
  - "Jacob Heye Hilbrands (12229285)"
  - "Mustafa Alsudani (1214099)"
  - "Moritz Renkin (11807211)"
  - "Nils Klüwer (12229263)"
  

abstract: "Credit scorecards are crucial tools in the credit assessment process. They are based on the prior performance of clients that share the same traits as a new client. Consequently, the goal of a credit scorecard is to estimate risk because credit scorecards are based on the past behavior of clients that share the same qualities as a prospective client. Therefore, the primary purpose of it is to either approve or reject a new client's loan request. The scorecard's function is to support this option. In this assignment, we'll compare and contrast the binning techniques known as weight-of-evidence (woe) binning and group binning. That’s why we have been using the existing scorecard. Moreover, certain model changes and binning splits will be made as a result of the addition of the 7 variables. To evaluate the effectiveness of the scorecard, the Gini coefficient is used. The major results of this research were to: demonstrate that group binning outperforms woe binning in out-of-sample predictions as measured by the Gini coefficient. The dependent variable, credibility, may be predicted more precisely when more independent variables are included."
date: "November, 2022"
output:
  pdf_document:
    toc: yes
    number_sections: TRUE
    citation_package: natbib
bibliography: literature.bib
biblio-style: natdin #humannat
include-before:
  \pagebreak
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

\newpage

# Abstract {.unnumbered}

Credit scorecards are crucial tools in the credit assessment process. They are based on the prior performance of clients that share the same traits as a new client. Consequently, the goal of a credit scorecard is to estimate risk because credit scorecards are based on the past behavior of clients that share the same qualities as a prospective client. Therefore, the primary purpose of it is to either approve or reject a new client's loan request. The scorecard's function is to support this option. In this assignment, we'll compare and contrast the binning techniques known as weight-of-evidence (woe) binning and group binning. That's why we have been using the existing scorecard. Moreover, certain model changes and binning splits will be made as a result of the addition of the 7 variables. To evaluate the effectiveness of the scorecard, the Gini coefficient is used. The major results of this research were to: demonstrate that group binning outperforms woe binning in out-of-sample predictions as measured by the Gini coefficient. The dependent variable, credibility, may be predicted more precisely when more independent variables are included.

# Introduction

In the form of a score and a likelihood of default, credit scoring models and scorecards estimate the risk that a borrower won't return a loan.

For instance, a credit scorecard may award a borrower points based on the following table for their age and income. As previously noted, a single dependent variable, credibility, was predicted using seven independent variables. As a binning strategy, the Weight-of-Evidence technique was adopted.

Therefore, group binning will be used as an extra binning strategy for this project's execution. To increase the model's Gini coefficient of prediction, a seventh independent variable was added, and the binning breaks were modified.

# Predictive Analytics Research Methodology

## Predictor Variable Transformation

Transforming predictor variables with the Weight of Evidence (WOE) approach. The WOE describes how well an independent variable may predict the outcome of a dependent variable.

The WOE cannot be determined until the data has been separated into bins. This value is used for future calculations in place of the original value once the WOE has been located. As a result, upon transformation, the WOE will be the same for all lines that include the variable in the same bin.

If the WOE is greater than one, then the Distribution of the Goods is greater than the Bads, if the WOE is smaller than one, then the Distribution of the Goods is less than the Distribution of the Bads.

There are benefits to this transformation: Because the data is categorized with the simple treatment of the missing values, outliers are no longer a concern. Additionally, it supports categorical and continuous numbers.

Using the two functions in R:

 woebin: This function determines the best binning.

 woebin_ply: This function applies the provided binning information to convert the values in the original data to WOE values. 

There will be some drawbacks for WOE as well. One is that binning may result in information loss. Another is the possibility of an unresearched link between the independent variables.


```{=tex}
\begin{align} \label{eq:WOE}
WOE = \frac{Distribution of Goods}{Distribution of Bads}
\end{align}
```
## Logistic Regression Analysis

Logistic Regression (also known as Logit-Model) is a statistical Model that can estimate probability of a certain event happening based on one or more independent variables. Its application is widespread in various statistical methods, especially in classification problems and prediction analyses. Contrary to linear regression, the predicted variable in logistic regression is a Bernoulli variable, i.e. a binary random variable $k$ with:

```{=tex}
\begin{align} \label{eq:bernoulli_var}
k \in \{0,1\}
\end{align}
```
Formally, the logistic regression model estimates probability $p$ of the Bernoulli $k$ being 1, corresponding the the event in question happening. The logistic function defining $p(x)$ takes on the form:

```{=tex}
\begin{align} \label{eq:VaR}
p(k) = \frac{e^{{\beta}_0 + {\beta}_1k}}{1 + e^{{\beta}_0 + {\beta_1}k}} = \frac{1}{1 + e^-({{\beta}_0 + {\beta_1}k})}
\end{align}
```
Figure TODO depicts the logistic regression for two examplary attributes.

In the domain of Credit Scoring, logistic regression is one of the most widely used statistical models. \cite[p.~19]{bolton2009logistic}

## Building Scorecards

lorem impsum

## Forecasting Scorepoints and default Probabilities

lorem impsum

## Forecasting Accuracy Testing

Include test-train split

# Empirical results

In this section the approach and the results of building a Out-of-Sample Gini coefficient maximizing scorecard model with only seven predictor variables are presented step-by-step. The scorecard is built on the "germancredit" data which is included in the "scorecard" library.


## Loading and Preparing the Data

The dataset "germancredit" is used from the library "scorecard". Below the import process is shown. The "germancredit" data has 21 variables int total. Seven predictor variables and a single dependent variable the "creditability" will remain for our scorecard model.

```{r echo=T}
# Importing of library and data
library(scorecard)
data("germancredit")
names(germancredit)
```

The first step of finding relevant predictor variables is done with the "var_filter()" function on the entire dataset. The default limits and rates for iv_limit, missing_rate & identical_rate mentioned below are used. The iv_limit excludes every variables whose information value is lower or equal to 0.02 in respect to our depedent variable "creditability". Through this filtering process 7 predictor variables are already excluded which are not eligible for the scorecard model as they cannot meet the explained criteria. In the "data_f.df" 14 variables are remaining, 13 possible predictor variables and one dependent variable "creditablitity". 

```{r echo=T, results='hide'}
# Filtering of variables with iv_limit >= 0.02, missing_rate <= 0.95 
# and identical_rate <= 0.95
data_f.df = var_filter(germancredit, y="creditability")

```

```{r echo=T, results='markup'}
ncol(data_f.df)
```


## Splitting the Data into Train and Test Samples

The remaining 13 possible predictor variables are splitted with the split_df function into train and test data with a ratio fo 75% train and 25% test data. Afterwards data_f.list is reformated to be useable in the later process. The following scorecard model is only trained on the train data. The test data is later used to validate and test the performance of the scorecard model.

```{r echo=T, results='hide'}
# Splitting data into train and test data with ratio 0.75
data_f.list = split_df(data_f.df,"creditability",ratio=c(0.75,0.25))
class(data_f.list)
lapply(data_f.list,class)
lapply(data_f.list, dim)
```

```{r include=FALSE, results='hide'}
# Generation a list for response variable: Dummy variable for defaults
default.list = lapply(data_f.list, function(x) x$creditability)
```

## Weight-Of-Evidence (WOE)-Binning

In the following the WOE-Binning is done with the 'woebin' function. For the 'woebin' function the default 'method="tree"' is used to generate the optimal binning of both numerical and categorical predictor variables. The breaks are generated automatically by the function and are saved in the "breaks.list" file.  


```{r echo=T, results='hide'}
# Binning of train data
# breaks.list is saved and imported seperately
bins.list = woebin(data_f.list$train,
                   "creditability",
                   save_breaks_list = "breaks.list")
```

This breaks.list file needs to be run as well to generate a report on the entire scoreboard model process later on.

```{r include=FALSE, results='hide'}
options(scorecard.bin_close_right = FALSE) 
breaks.list=list(
 status.of.existing.checking.account=c("... < 0 DM%,%0 <= ... < 200 DM", "... >= 200 DM / salary assignments for at least 1 year", "no checking account"), 
 duration.in.month=c("8", "16", "34", "44"), 
 credit.history=c("no credits taken/ all credits paid back duly%,%all credits at this bank paid back duly", "existing credits paid back duly till now", "delay in paying off in the past", "critical account/ other credits existing (not at this bank)"), 
 purpose=c("retraining%,%car (used)%,%radio/television", "furniture/equipment%,%repairs", "car (new)%,%domestic appliances%,%business", "education%,%others"), 
 credit.amount=c("1400", "4000", "5000", "9600"), 
 savings.account.and.bonds=c("... < 100 DM", "100 <= ... < 500 DM", "500 <= ... < 1000 DM%,%... >= 1000 DM%,%unknown/ no savings account"), 
 present.employment.since=c("unemployed%,%... < 1 year", "1 <= ... < 4 years", "4 <= ... < 7 years%,%... >= 7 years"), 
 installment.rate.in.percentage.of.disposable.income=c("2", "3", "4"), 
 other.debtors.or.guarantors=c("none%,%co-applicant", "guarantor"), 
 property=c("real estate", "building society savings agreement/ life insurance", "car or other, not in attribute Savings account/bonds", "unknown / no property"), 
 age.in.years=c("26", "30", "35", "39"), 
 other.installment.plans=c("bank%,%stores", "none"), 
 housing=c("rent", "own", "for free")
 )
```

Below the binning of the seven predictor variables with the highest information value is shown in descending order. This does not mean that these values are the final seven predictor variables. The full plots of all predictor variables can be viewed in the Appendix. 

```{r fig.height=2, fig.align='center', echo=FALSE}
woebin_plot(bins.list$status.of.existing.checking.account)
woebin_plot(bins.list$credit.history)
woebin_plot(bins.list$duration.in.month)
woebin_plot(bins.list$credit.amount)
woebin_plot(bins.list$purpose)
woebin_plot(bins.list$age.in.years)
woebin_plot(bins.list$savings.account.and.bonds)
```

## Transformation of predictor variables

The train and test data is transformed with the generated information by 'woebin' into WOE-based predictor variables. These can be used later in glm and scorecard building. This is done by using the function 'woebin_ply'.

```{r echo=TRUE, results='hide'}
# WOE-Transformation of train and test data
data_woe.list = lapply(data_f.list,
                       function(x) woebin_ply(x, bins.list))
lapply(data_woe.list, class)
lapply(data_woe.list, dim)
```

In addition, the train and test data can be transformed into Bin-Group (GRP) based predictor variables using the bin borders or breaks. The functionality of building a scorecard with the GRP-Transforamtion is not yet implemented in the package at the current state, but the GRP-Transformation is later used for comparison and validation.

```{r echo=TRUE, results='hide'}
# Bin-Group (GRP) Transformation of train and test data
data_grp.list = lapply(data_f.list, 
                       function(x) woebin_ply(x, bins.list, to = 'bin'))
lapply(data_grp.list, class)
lapply(data_grp.list, dim)
```

## Generalized linear model (glm): Regressing response w.r.t. predictors

In this section, the logistic regression models are built. These models are used to select the seven most significant variables and subsequently to build the scorecard model with the chosen variables.

### Selection of seven variables with Logistic regression w.r.t. WOE-transformed predictors
In order to select the seven most significant variables, a logistic regression is performed on the WOE-transformed predictor variables of the train data. The seven predictor variables which have the lowest significance value, are selected.

```{r echo=TRUE, message=FALSE, warning=FALSE}
data_woe_first_iteration.glm <- glm(creditability ~ .,
                                    family = binomial(),
                                    data = data_woe.list$train)
summary(data_woe_first_iteration.glm)
```

The following seven predictor variables are selected as they have the lowest significance value (<0.01):
\begin{itemize}
\item{status.of.existing.checking.account}
\item{duration.in.month}
\item{credit.history}
\item{purpose}
\item{credit.amount}
\item{savings.account.and.bonds}
\item{age.in.years}
\end{itemize}

### Logistic regression w.r.t. selected WOE-transformed predictors

The logistic regression model with the seven selected WOE-transformed predictor variables is built on the train data and saved in "data_woe_second_iteration.glm".

```{r, echo=TRUE}
data_woe_second_iteration.glm <- glm(creditability ~ 
                                      status.of.existing.checking.account_woe
                                      + duration.in.month_woe
                                      + credit.history_woe
                                      + purpose_woe
                                      + credit.amount_woe
                                      + savings.account.and.bonds_woe
                                      + age.in.years_woe,
                                        family = binomial(),
                                        data = data_woe.list$train)
```

### Logistic regression w.r.t. selected GRP-transformed predictors

The logistic regression model with the seven selected GRP-transformed predictor variables is built on the train data and saved in "data_grp_second_iteration.glm".

```{r, echo=TRUE}
data_grp_second_iteration.glm <- glm(creditability ~ 
                                      status.of.existing.checking.account_bin
                                      + duration.in.month_bin
                                      + credit.history_bin
                                      + purpose_bin
                                      + credit.amount_bin
                                      + savings.account.and.bonds_bin
                                      + age.in.years_bin,
                                     family = binomial(),
                                     data = data_grp.list$train)
```

## Building the scorecard-model

The scorecard is built with the logistic regression model of the seven WOE-transformed variables and is saved in "scorecard_woe_second_iteration.scm". The scorecard contains the baseline points and the points associated with every bin. In the output of the command `scorecard_woe_second_iteration.scm$purpose`, the points for each bin of the "purpose" predictor variable is displayed. When the purpose of a credit is e.g. furniture, equipment or repairs, the credit applicant receives -3 points for this predictor variable.

```{r, echo=TRUE}
scorecard_woe_second_iteration.scm <- scorecard(bins.list,
                                               data_woe_second_iteration.glm)
names(scorecard_woe_second_iteration.scm)
scorecard_woe_second_iteration.scm$purpose
```

The scores for the entire "germancredit" data are calculated and saved in "score_woe_second_iteration.df"

```{r, echo=TRUE}
score_woe_second_iteration.df = scorecard_ply(germancredit,
                                             scorecard_woe_second_iteration.scm,
                                             only_total_score = FALSE)
```

The scores for the splitted "germancredit" data (train and test) are calculated and saved in "score_woe_second_iteration.list".

```{r, echo=TRUE}
score_woe_second_iteration.list <- lapply(data_f.list,
                                         function(x) scorecard_ply(x, 
                                            scorecard_woe_second_iteration.scm))
```

A report can also be generated for this scorecard model which includes information on the dataset, model coefficients, model performance, WOE binning, scorecard, population stability, and gains.

```{r echo=TRUE,results='hide', fig.show='hide'}
# Report of Scoreboard with WOE-transformed predictor variables
y<-"creditability"
x<-c("status.of.existing.checking.account","duration.in.month","credit.history",
      "purpose","credit.amount","savings.account.and.bonds",
      "age.in.years")

report(data_f.list,
       y,
       x,
       breaks.list,
       seed = NULL,
       save_report = "Report_WOE_second_iteration")
```

Building a scorecard with GRP-transformed predictor variables is unfortunately not supported by the "scorecard" library. Therefore, no report can be created as well. Nevertheless, the performance of the logistic model of the GRP-transformed predictor variables is compared with the logistic model of the WOE-transformed predictor variables in the next section.

## Predicted probabilities and scorepoints

With the scorecard of the logistic regressions of both WOE- and GRP-transformed predictors and the WOE-transformed predictor variables, the predicted probabilities and scorepoints can be calculated. The predicted probabilities forecast the probability of default for each applicant in the "germancredit" data. The first predicted probabilities of both transformations are displayed below.

```{r echo=TRUE}
# Predicted probabilties w.r.t. WOE-transformed predictors
predProb_woe.list <- lapply(data_woe.list,
                        function(x) predict(data_woe_second_iteration.glm,
                                            type = 'response',
                                            x)
)
head(predProb_woe.list$train)
```

```{r echo=TRUE}
# Predicted probabilties w.r.t. GRP-transformed predictors
predProb_grp.list <- lapply(data_grp.list,
                            function(x) predict(data_grp_second_iteration.glm,
                                                type = 'response',
                                                x)
)
head(predProb_grp.list$train)
```

The calculated scorepoints of the applicants of the "germancredit" data can only be calculated for WOE-transformed predictor variables because no scorecard could be built for the GRP-transformed predictors.

```{r echo=TRUE}
# Predicted scorepoints w.r.t. WOE-transformed predictors
head(score_woe_second_iteration.list$train)
```

From the report that was created above for the scorecard with WOE-transformed predictor variables, it can be can be seen that the target score is 600 and the target predicted probability is 5,26%. That means, that applicants with a score below 600 and with a predicted probability below 5,26& are declined. 

## Gini Coefficient In-Sample and Out-of-Sample
Below the prediction power of the proposed predictor variables is validated through the In-Sample and Out-of-Sample testing. The full validation is done with the predicted scores in "score_woe_second_iteration.list" and the function perf_eva. To further validate the scorecard the Gini-Coefficient of "second_woe_second_iteration.list", "data_woe_second_iteration.glm" and "data_grp_second_iteration.glm" is calculated with the perf_eva function. 

```{r echo=TRUE}
#Scorecard Model second iteration
perf_eva(pred = score_woe_second_iteration.list,
         label = default.list,
         binomial_metric = c("gini","auc","r2", "rmse"),
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE)
```


```{r echo=TRUE}
#Second iteration GLM with WOE-Binning 
predProb_woe.list <- lapply(data_woe.list,
                        function(x) predict(data_woe_second_iteration.glm,
                                            type = 'response',
                                            x)
)
perf_eva(pred = predProb_woe.list,
         label = default.list,
         binomial_metric = c("gini"),
         show_plot=c(),
         confusion_matrix = FALSE)
```


```{r echo=TRUE}
#Second iteration GLM with GRP-Binning
predProb_grp.list <- lapply(data_grp.list,
                            function(x) predict(data_grp_second_iteration.glm,
                                                type = 'response',
                                                x)
)
perf_eva(pred = predProb_grp.list,
         label = default.list,
         binomial_metric = c("gini"),
         show_plot=c(),
         confusion_matrix = FALSE)
```

|Method | In-Sample Gini-Coefficient | Out-of-Sample Gini-Coefficient |
 | -------- | -------- | ------- |
|Second iteration Scorecard | 0.6436704 | 0.5348129	 |
|Second iteration WOE-Binning | 0.6444203 | 0.5346227	 |
|Second iteration GRP-Binning | 0.6502033 | 0.5352568 |

As expected the In-Sample Gini-Coefficient is much higher as the OoS-Gin-Coeficient, but still performing well with over 50%. The differences in the OoS testing are marginal raging from 0.5346227 (WOE-Binning) to 0.5352568 (GRP-Binning) having only a difference of 0.0006341. As stated by Peussa (2016) the OoS-Gini Coefficient lies between 10 and 50%, having 53.53% it could be shown that the approach used works.
(QUELLE: CREDIT RISK SCORECARD ESTIMATION BY LOGISTIC
REGRESSION, Aleksandr Peussa, May 2016)

# Summary

TODO

# Appendix

## Full List of WOE-Binnined predictor variables

```{r fig.height=2, fig.align='center', echo=FALSE}
woebin_plot(bins.list)
```





