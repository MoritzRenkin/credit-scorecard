---
title:  '\begin{center} Predictive Analytics: Credit Risk Scorecard Application \\ Case Study: Group 15 \\  \end{center}'
author:
  - "Jacob Heye Hilbrands (12229285)"
  - "Mustafa Alsudani (1214099)"
  - "Moritz Renkin (11807211)"
  - "Nils Klüwer (12229263)"
  

abstract: "Credit scorecards are crucial tools in the credit assessment process. They are based on the prior performance of clients that share the same traits as a new client. Consequently, the goal of a credit scorecard is to estimate risk because credit scorecards are based on the past behavior of clients that share the same qualities as a prospective client. Therefore, the primary purpose of it is to either approve or reject a new client's loan request. The scorecard's function is to support this option. In this assignment, we'll compare and contrast the binning techniques known as weight-of-evidence (woe) binning and group binning. That’s why we have been using the existing scorecard. Moreover, certain model changes and binning splits will be made as a result of the addition of the 7 variables. To evaluate the effectiveness of the scorecard, the Gini coefficient is used. The major results of this research were to: demonstrate that group binning outperforms woe binning in out-of-sample predictions as measured by the Gini coefficient. The dependent variable, credibility, may be predicted more precisely when more independent variables are included."
date: "November, 2022"
output:
  pdf_document:
    toc: yes
    number_sections: TRUE
    citation_package: natbib
bibliography: literature.bib
biblio-style: natdin #humannat
include-before:
  \pagebreak
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

\newpage

# Abstract {-}

Credit scorecards are crucial tools in the credit assessment process. They are based on the prior performance of clients that share the same traits as a new client. Consequently, the goal of a credit scorecard is to estimate risk because credit scorecards are based on the past behavior of clients that share the same qualities as a prospective client. Therefore, the primary purpose of it is to either approve or reject a new client's loan request. The scorecard's function is to support this option. In this assignment, we'll compare and contrast the binning techniques known as weight-of-evidence (woe) binning and group binning. That’s why we have been using the existing scorecard. Moreover, certain model changes and binning splits will be made as a result of the addition of the 7 variables. To evaluate the effectiveness of the scorecard, the Gini coefficient is used. The major results of this research were to: demonstrate that group binning outperforms woe binning in out-of-sample predictions as measured by the Gini coefficient. The dependent variable, credibility, may be predicted more precisely when more independent variables are included.

# Introduction

In the form of a score and a likelihood of default, credit scoring models and scorecards estimate the risk that a borrower won't return a loan.

For instance, a credit scorecard may award a borrower points based on the following table for their age and income. As previously noted, a single dependent variable, credibility, was predicted using seven independent variables. As a binning strategy, the Weight-of-Evidence technique was adopted.

Therefore, group binning will be used as an extra binning strategy for this project's execution. To increase the model's Gini coefficient of prediction, a seventh independent variable was added, and the binning breaks were modified.


# Predictive Analytics Research Methodology

## Predictor Variable Transformation

 Transforming predictor variables with the Weight of Evidence (WOE) approach. The WOE describes how well an independent variable may predict the outcome of a dependent variable. 

The WOE cannot be determined until the data has been separated into bins. This value is used for future calculations in place of the original value once the WOE has been located. As a result, upon transformation, the WOE will be the same for all lines that include the variable in the same bin. 

If the WOE is greater than one, then the Distribution of the Goods is greater than the Bads, if the WOE is smaller than one, then the Distribution of the Goods is less than the Distribution of the Bads. 

There are benefits to this transformation: Because the data is categorized with the simple treatment of the missing values, outliers are no longer a concern. Additionally, it supports categorical and continuous numbers.

```{=tex}
\begin{align} \label{eq:WOE}
WOE = \frac{Distribution of Goods}{Distribution of Bads}
\end{align}
```
## Logistic Regression Analysis

Logistic Regression (also known as Logit-Model) is a statistical Model that can estimate probability of a certain event happening based on one or more independent variables. Its application is widespread in various statistical methods, especially in classification problems and prediction analyses. Contrary to linear regression, the predicted variable in logistic regression is a Bernoulli variable, i.e. a binary random variable $k$ with:

```{=tex}
\begin{align} \label{eq:bernoulli_var}
k \in \{0,1\}
\end{align}
```
Formally, the logistic regression model estimates probability $p$ of the Bernoulli $k$ being 1, corresponding the the event in question happening. The logistic function defining $p(x)$ takes on the form:

```{=tex}
\begin{align} \label{eq:VaR}
p(k) = \frac{e^{{\beta}_0 + {\beta}_1k}}{1 + e^{{\beta}_0 + {\beta_1}k}} = \frac{1}{1 + e^-({{\beta}_0 + {\beta_1}k})}
\end{align}
```

Figure TODO depicts the logistic regression for two exeplamary attributes.

In the domain of Credit Scoring, logistic regression is one of the most widely used statistical models. \cite[p.~19]{Bolton2010LogisticRA}


## Building Scorecards

lorem impsum

## Forecasting Scorepoints and default Probabilities

lorem impsum

## Forecasting Accuracy Testing

lorem impsum

# Empirical results

lorem impsum

## Loading external data

 requesting the loading of the "germancredit" built-in data set while creating the scorecard, we may leverage a variety of variables in our project to forecast creditability based on a subset of these variables. 
 
```{r}
# Importing of library and data
library(scorecard)
data("germancredit") 
```
In order to achieve the aim of predicting creditability, we will take into account 7 variables from the "germancredit" data and determine which one has the most influence on the precision of a predicted desired value.
Considering the Filtering of variables with >= iv_limit = 0.02, <= missing_limit = 0.95 
 and <= identical_limit_limit = 0.95.
```{r}
data_f.df = var_filter(germancredit, y="creditability")
```

## Splitting filtered data into train and test samples

It is essential to draw conclusions from historical data and use the information gained to make precise predictions about credibility in order to have a scorecard that can be relied upon. As a result, we must divide the data into two sets that we loaded and filtered in the earlier phases. The sets consist of a training set and a test set, with the training set accounting for 75% and the test set for 25% of the total.

```{r}
data_f.list = split_df(data_f.df,"creditability",ratio=c(0.75,0.25))
```

The model can make a forecast that is as accurate as feasible since there is enough training data available. To determine if the learning process was sufficiently effective, the test set is employed.

## Weight-Of-Evidence (WOE)-Binning


Using the two functions in R: woebin & woebin_ply. As shown below: 

-	WOE-Binning of predictor variables (used method: “width”):

bins.list = woebin(data_f.list$train,
                   "creditability",
                   save_breaks_list = "breaks.list")
head(bins.list)
woebin_plot (bins.list)

-	Transformation of predictor variables WOE and GRP: 
      
  data_woe.list = lapply(data_f.list,
                       function(x) woebin_ply(x, bins.list))
lapply(data_woe.list, class)
lapply(data_woe.list, dim) 

data_grp.list = lapply(data_f.list,
                       function(x) woebin_ply(x, bins.list, to = 'bin'))
lapply(data_grp.list, class)
lapply(data_grp.list, dim)
## Generalized linear model (glm): Regressing response w.r.t. predictors

lorem impsum

### Logistic regression w.r.t. WOE-transformed predictors (data_woe.list\$train)

lorem impsum

### Logistic regression w.r.t. GRP-transformed predictors (data_grp.list\$train)

lorem impsum

## Building the scorecard-model

lorem impsum

### Calculating scorepoints for the splitted sample (train and test)

lorem impsum

## Predicting probabilities and scorepoints

lorem impsum

## Testing prediction accuracy

lorem impsum

# Summary

lorem impsum

````{=html}
<!-- -----------------------------------------------------------
FROM TEMPLATE
# Contextualization: Loading and transforming data

Origins of Weight-Of-Evidence: \cite{Good}, \cite{Kullback}

URLs of web publications:\
<https://www.rdocumentation.org/packages/scorecard/versions/0.1.8>\
<https://rpubs.com/ngyongkad/scorecard>\
<https://stats.stackexchange.com/questions/419160/credit-scorecard-using-logistic-regression-on-r>

## Methodological and linguistic overview

In the credit risk domain two predicting models will be of special importance, i.e. Generalized Linear Models (glm) and Score Card Models (scm).

LVA: "Risk Model Management" (summer term by Dr. Lederer) by using the construction, calibration and validation (CCV) framework.

## Loading libraries

```{r}
library(scorecard)
```

## Loading external data

The variables are distinguished among predictor (feature, independent) variables and response (label, dependent) variables.

One response variable: creditability\
Five predictor variables: credit.amount, duration.in.month, credit.history, purpose, property

```{r}
data("germancredit") 
data.df<-germancredit[,c("creditability",
                      "credit.amount","duration.in.month","credit.history",
                      "purpose","property")]
head(data.df[,1:3])
head(data.df[,4])
head(data.df[,5])
str(data.df)
```

Hint: Consider the different primitive data types in R, i.e. numeric (num), factor (Factor), character (chr) and integer (int).

Hint: Consider the different composed data types in R, i.e. vector, matrix, array, data frame (df) and list. Theses types will be indicated in the names of the variables, e.g. data.df is a data frame that contains the data.

## Filtering data and transforming data types

Filtering for missing values, information values and identical values by using the var_filter() function

```{r}
data_f.df <- var_filter(data.df, 
                        "creditability")
str(data_f.df)
```

Hint: Consider the change of the "creditability" data type from "factor" to "integer"

------------------------------------------------------------------------

Calculating the information value of the predictor variables for selecting "informative" variables

```{r}
iv(data.df,y="creditability")
```

## Splitting filtered data into train and test samples

Splitting the filtered data frame into a list that contains data frames for the train and the test samples

```{r}
data_f.list <- split_df(data_f.df, 
                       "creditability")
class(data_f.list)
lapply(data_f.list,class)
lapply(data_f.list, dim)
str(data_f.list$train)
```

Hint: By default the splitting is 70/30 % for the train/test samples.It can be altered in the split_df() function by the argument "ratio=c(0.7,0.3)". The names of the splitted samples can be altered by the argument "name_dfs=c('train','test')."

------------------------------------------------------------------------

Generating a list for response variable: Dummy variable for defaults

```{r}
default.list = lapply(data_f.list, function(x) x$creditability)
```

\newpage

# Transforming the predictor variables 'x': WOE(x) and GRP(x)

## Weight-Of-Evidence (WOE)-Binning

Grouping (binning) predictor variables with respect to (w.r.t.) the WOE metric

### WOE-Binning of total data: bins.df

Grouping all, i.e. total data: Needed for total cross validation purposes

```{r}
bins.df = woebin(data_f.df, 
                 "creditability") 
```

Hint: The default binning method is method="width". Other methods are "frequ" that support numerical variables as well as "tree" and "chimerge" supporting both, i.e. numerical and categorical variables which are used in the optimal binning approach.

------------------------------------------------------------------------

### WOE-Binning of train and test data: bins.list

Grouping only train data: Needed for separated train/test analysis

```{r}
bins.list = woebin(data_f.list$train, 
                   "creditability",
                   save_breaks_list = "breaks.list") 
```

Hint: The breaks list is saved, because it is needed to build a report for the scorecard modeling. As the break list is saved in a separate R-script file, this file has to be loaded and the break list is saved as a variable that can be used in the scorecard modeling report.

```{r}
breaks.list=list(
 credit.amount=c("1400", "4000", "5000", "9600"), 
 duration.in.month=c("8", "16", "26", "44"), 
 credit.history=c("no credits taken/ all credits paid back duly%,%all credits at this bank paid back duly", "existing credits paid back duly till now", "delay in paying off in the past", "critical account/ other credits existing (not at this bank)"), 
 purpose=c("retraining%,%car (used)%,%radio/television", "furniture/equipment", "repairs%,%car (new)%,%business", "domestic appliances%,%education%,%others"), 
 property=c("real estate", "building society savings agreement/ life insurance", "car or other, not in attribute Savings account/bonds", "unknown / no property")
 )
```

------------------------------------------------------------------------

Plotting the bins (including bin statistics)

```{r fig.height=2.5, fig.align='center'}
woebin_plot(bins.list)
```

## Bin-WOE transformation of predictor variables

### Bin-WOE transformation of total data: data_woe.df

Transforming total sample: Needed for cross validation purposes

```{r}
data_woe.df <- woebin_ply(data_f.df, bins.df)
```

------------------------------------------------------------------------

### Bin-WOE transformation of train/test data: data_woe.list

Transforming splitted sample: Needed for train/test analysis

```{r}
data_woe.list <- lapply(data_f.list, 
                       function(x) woebin_ply(x, bins.list))
lapply(data_woe.list, class)
lapply(data_woe.list, dim)
head(data_woe.list$train[,1:3])
```

## Bin-Group (GRP) transformation of predictor variables

```{r}
data_grp.list = lapply(data_f.list, 
                       function(x) woebin_ply(x, bins.list, to = 'bin'))
lapply(data_grp.list, class)
lapply(data_grp.list, dim)
head(data_grp.list$train[,1:3])
head(data_grp.list$train[,4])
```

\newpage

# Generalized linear model (glm): Regressing response w.r.t. predictors

## Logistic regression w.r.t. original predictors (data_f.list\$train)

### Logistic regression: Including original information of numeric predictors only

For comparison to lm modeling see e.g. <http://www.learnbymarketing.com/tutorials/explaining-the-lm-summary-in-r/>

```{r}
summary(glm(creditability ~ credit.amount + duration.in.month, 
    family = binomial(), 
    data = data_f.list$train))
```

Hint: The selection of the train sample in the logistic regression model is important for being able to implement Out-of-Sample (OoS) tests for assessing the forecasting accuracy of the glm-models and the thereof derived scorecard-models.

### Logistic regression: Including original information of numeric, categorical and character predictors

```{r}
data_f.glm <- glm(creditability ~ ., 
                  family = binomial(), 
                  data = data_f.list$train) 
head(data_f.glm$model)
```

------------------------------------------------------------------------

Regression results

```{r}
#summary(data_f.glm)
names(summary(data_f.glm))
head(data_f.glm$coefficients)
```

------------------------------------------------------------------------

Analysis of regression via "analyses of variances": anova()

```{r}
anova(data_f.glm)
```

------------------------------------------------------------------------

Analysis of regression via "variance inflation factors": vif()

```{r}
vif(data_f.glm, merge_coef = TRUE)
```

## Logistic regression w.r.t. WOE-transformed predictors (data_woe.list\$train)

The WOE-based logistic regression is the prefered regression approach as it delivers the most compact regression models.

### WOE-based logistic regression: Including numeric, categorical and character predictors

```{r}
data_woe.glm <- glm(creditability ~ ., 
                    family = binomial(), 
                    data = data_woe.list$train) 
head(data_woe.glm$model)
```

------------------------------------------------------------------------

Summary of regression: summary()

```{r}
summary(data_woe.glm)
names(summary(data_woe.glm))
data_woe.glm$coefficients
```

------------------------------------------------------------------------

Analyses of variance: anova()

```{r}
anova(data_woe.glm)
```

------------------------------------------------------------------------

Variance inflation factors: vif()

```{r}
vif(data_woe.glm, merge_coef = TRUE)
```

### WOE-based logistic regression: Automatic selection of the best AIC-model

Automatic selection of optimal regression model via the Akaike Information Criteria (AIC)

```{r}
glm_step <- step(data_woe.glm, 
                 direction="both", 
                 trace=FALSE) 
summary(eval(glm_step$call))
```

### WOE-based logistic regression: Including numeric predictors only

Numeric predictors: Regression with predictor variables that are of numeric data type

```{r}
summary(glm(creditability ~ credit.amount_woe + duration.in.month_woe, 
                    family = binomial(), 
                    data = data_woe.list$train)) 
```

## Logistic regression w.r.t. GRP-transformed predictors (data_grp.list\$train)

### GRP-based logistic regression: Including GRP-information of numeric predictors only

Categorical regression type: Taking bin-groups as categorical variables (dummy variable representation)

```{r}
summary(glm(creditability ~ credit.amount_bin + duration.in.month_bin, 
    family = binomial(), 
    data = data_grp.list$train))
```

### GRP-based logistic regression: Including numeric, categorical and character predictors

```{r}
data_grp.glm <- glm(creditability ~ ., 
                    family = binomial(), 
                    data = data_grp.list$train) 
head(data_grp.glm$model)
```

------------------------------------------------------------------------

```{r}
#summary(data_grp.glm)
names(summary(data_grp.glm))
head(data_grp.glm$coefficients)
```

------------------------------------------------------------------------

```{r}
anova(data_grp.glm)
```

### Transforming categorical and character predictors into factors with the one_hot() function

```{r}
data_grp_1h.list<-list()
data_grp_1h.list$train <- one_hot(data_grp.list$train, 
                                  var_encode = c("credit.history_bin",
                                                 "purpose_bin","property_bin")
                                  )
data_grp_1h.list$test <- one_hot(data_grp.list$test, 
                                 var_encode = c("credit.history_bin", 
                                                "purpose_bin", "property_bin")
                                 )
names(data_grp_1h.list)
lapply(data_grp_1h.list,dim)
```

\newpage

# Building the scorecard-model (scm) and calculating scorepoints

Scorepoints are calculated by combing scorecard-model, which combines bin and glm information, with individual data

## Building the scorecard-model

Building the scorecard via bin and glm information resulting from train sample

```{r}
scorecard.scm <- scorecard(bins.list,
                       data_woe.glm)
names(scorecard.scm)
```

------------------------------------------------------------------------

Investigating the content of the scorecard

```{r}
scorecard.scm$basepoints
scorecard.scm$credit.amount
```

## Calculating the scorepoints by combinig scorecard-model and individual data

Calculating the scorepoints (scores)

```{r}
score.df = scorecard_ply(data.df,
                         scorecard.scm, 
                         only_total_score = FALSE)
names(score.df)
head(score.df)
```

## Calculating scorepoints for the splitted sample (train and test)

Generating a score list

```{r}
score.list <- lapply(data_f.list, 
                     function(x) scorecard_ply(x, scorecard.scm)) 
names(score.list)
```

Hint: The only_total_score = TRUE (= default argument) has to be used for providing two compatible lists for further processing.

## Report (saved spreadsheet format): Scorecard modeling

```{r eval=FALSE}
# {r include=FALSE}
y<-"creditability"
x<-c("credit.amount","duration.in.month","credit.history","purpose","property")
# breaks.list as defined in the woebin()
report(data_f.list, 
       y, 
       x, 
       breaks.list, 
       seed = NULL, 
       save_report = "Report01")
```

Hint: Generated report file is stored in xlsx-format.\
Hint: The R-chunk is not included (i.e. {r include=FALSE}) as otherwise the report would be included in the pdf-file as well.

\newpage

# Predicting (forecasting) probabilities and scorepoints

## Probability prediction for the sub-samples (train and test)

```{r}
predProb.list <- lapply(data_woe.list, 
                        function(x) predict(data_woe.glm, 
                                            type = 'response', 
                                            x)
                        )
names(predProb.list)
```

Hint: Due to the fact that the data_woe.glm was calibrated for the train sample two different types of prediction can be destinguished, i.e. the in-sample (IS) prediction by using the train sample in the predict()-function, and the out-of-sample (OoS) prediction by using the test sample in the predict()-function.

```{r}
head(predProb.list$train) # In-Sample prediction
head(predProb.list$test)  # Out-of-Sample prediction
```

## Scorepoint prediction for the sub-samples (train and test)

The prediction of the scorepoints is alread incorported in the built scorecard.

```{r}
head(score.list$train)
head(score.list$test)
```

\newpage

# Analyzing scoring results and testing forecasting accuracy

## Stability of score distributions: Population stability index (PSI)

fig.width=5

```{r fig.height=2.5}
psi.list <- perf_psi(score = score.list, 
                     label = default.list,
                     return_distr_dat = TRUE)
names(psi.list)
names(psi.list$dat)
head(psi.list$dat$score[,1:9])
psi.list$psi
psi.list$pic
```

Hint: More details of per_psi() function are given \@ <https://www.rdocumentation.org/packages/scorecard/versions/0.1.9/topics/perf_psi>

```{r eval=FALSE}
perf_psi(score, label = NULL, title = NULL, x_limits = NULL,
  x_tick_break = 50, show_plot = TRUE, seed = 186,
  return_distr_dat = FALSE)
# e.g. # x_limits = c(250, 700),
#      # x_tick_break = 50,
```

## Statistical analysis of scoring system

```{r}
gains.df <- gains_table(score = score.list, 
                        label = default.list,
                        method = "width")
```

Investigating the gains dataframe

```{r}
head(gains.df)
tail(gains.df)
```

## Cross validation of total and sub-samples

### Cross validation w.r.t. total data

```{r}
cv.list <- perf_cv(data_woe.df, 
                   y='creditability', 
                   no_folds = 5, 
                   binomial_metric = c('ks','auc','gini','r2'))
cv.list
```

### Cross validation w.r.t. train data

```{r}
cv.list01 <- perf_cv(data_woe.list$train, 
                   y='creditability', 
                   no_folds = 5, 
                   binomial_metric = c('ks','auc','gini','r2'))
cv.list01
```

## Probability prediction accuracy (single dataset): In-Sample and Out-of-Sample testing

### IS-testing of predicted probabilities via train sample

```{r fig.height=2.5}
perf_eva(pred = predProb.list$train, 
         label = data_woe.list$train$creditability, 
         title = 'train',
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE) 
```

Hint: Threshold of confusion matrix relates to the probability of default.

### OoS-testing of predicted probabilities via test sample

```{r fig.height=2.5}
perf_eva(pred = predProb.list$test, 
         label = data_woe.list$test$creditability, 
         title = 'test',
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE) 
```

## Scorepoint prediction accuracy (single dataset): In-Sample and Out-of-Sample testing

### IS-testing of predicted scores via train sample

```{r fig.height=2.5}
perf_eva(pred = score.list$train, 
         label = data_f.list$train$creditability, 
         title = 'train',
         binomial_metric = c("mse","rmse","logloss","r2", "ks","auc","gini"),
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE)
```

Hint: Threshold of confusion matrix relates to the scores.

### OoS-testing of predicted scores via test sample

```{r fig.height=2.5}
perf_eva(pred = score.list$test, 
         label = data_f.list$test$creditability, 
         title = 'test',
         binomial_metric = c("gini","auc","r2", "rmse"),
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE)
```

## Prediction accuracy (multiple dataset): IS- and OoS-Testing in one

Predicted probabilities

```{r fig.height=2.5}
perf_eva(pred = predProb.list, 
         label = default.list,
         binomial_metric = c("gini","auc","r2", "rmse"),
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE)
```

Predicted scores

```{r fig.height=2.5}
perf_eva(pred = score.list, 
         label = default.list,
         binomial_metric = c("gini","auc","r2", "rmse"),
         show_plot=c("roc","ks"),
         confusion_matrix = TRUE)
```

\newpage

# Appendix

## Appendix: Essay style with formulas in LaTeX language

**Group project assignment**: Write a scholarly essay with full sentences, correct citations and LaTeX formulas.

**Example essay style**: From a statistical perspective the transition from the $MPS$ to the VaR framework is related to switching the perspective from considering moments (parameters) of random variables, i.e. $\mu$ and $\sigma$, to considering the quantiles and corresponding probabilities of these variables. Specifically, the VaR measure specifies the risk of a random variable ($\tilde{P}$) via the threshold quantile ($VaR$) that is exceeded into the negative direction (i.e. $P \leq VaR$) with the loss probability ($\alpha$) or respectively, is exceeded into the positive direction (i.e. $P > VaR$) with the complementary probability, i.e. the confidence level ($1-\alpha$).

```{=tex}
\begin{align} \label{eq:VaR}
\mathrm{Pr}\{ \tilde{P} \leq VaR \} = \alpha
\end{align}
```
In formula (\ref{eq:VaR}) the definition of the Value-at-Risk (VaR) is given.

\newpage

```{r eval=FALSE}
# Problem: No scorecards can be built w.r.t. grp-binned data
# but logistic regression works fine with grp-binned data
# usind "Probability" prediction only for analysing grp-binned data

scorecard_grp.scm <- scorecard(bins.list,
                       data_grp.glm)
names(scorecard_grp.scm)
scorecard_grp.scm$basepoints
scorecard_grp.scm$credit.amount
scorecard.scm$basepoints
scorecard.scm$credit.amount
```

\newpage
-->
````

\`\`\`\`
